{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "ENEpY7G7YJrG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nyJhBy8gLfm",
        "outputId": "81cf93a3-076f-4d32-9813-b51aedb13c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accelerometer shape: (87, 268, 3)\n",
            "Training Gyroscope shape: (87, 268, 3)\n",
            "Training Labels shape: (87,)\n",
            "Testing Accelerometer shape: (90, 268, 3)\n",
            "Testing Gyroscope shape: (90, 268, 3)\n",
            "Testing Labels shape: (90,)\n"
          ]
        }
      ],
      "source": [
        "# Load filtered training data\n",
        "tr_msAcc = np.load(\"train_MSAccelerometer_OpenDoor_RubHands.npy\")\n",
        "tr_msGyr = np.load(\"train_MSGyroscope_OpenDoor_RubHands.npy\")\n",
        "tr_labels = np.load(\"train_labels_OpenDoor_RubHands.npy\")\n",
        "\n",
        "# Load filtered testing data\n",
        "ts_msAcc = np.load(\"test_MSAccelerometer_OpenDoor_RubHands.npy\")\n",
        "ts_msGyr = np.load(\"test_MSGyroscope_OpenDoor_RubHands.npy\")\n",
        "ts_labels = np.load(\"test_labels_OpenDoor_RubHands.npy\")\n",
        "\n",
        "# Check the shapes of the loaded data\n",
        "print(\"Training Accelerometer shape:\", tr_msAcc.shape)\n",
        "print(\"Training Gyroscope shape:\", tr_msGyr.shape)\n",
        "print(\"Training Labels shape:\", tr_labels.shape)\n",
        "\n",
        "print(\"Testing Accelerometer shape:\", ts_msAcc.shape)\n",
        "print(\"Testing Gyroscope shape:\", ts_msGyr.shape)\n",
        "print(\"Testing Labels shape:\", ts_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of training and testing examples\n",
        "train_num_examples = 87\n",
        "test_num_examples = 90\n",
        "\n",
        "num_sensors = 2\n",
        "num_features_per_axis = 8\n",
        "num_axes = 3\n",
        "\n",
        "train_features = np.zeros((train_num_examples, num_sensors * num_features_per_axis, num_axes))\n",
        "test_features = np.zeros((test_num_examples, num_sensors * num_features_per_axis, num_axes))"
      ],
      "metadata": {
        "id": "RGWGasN1cuun"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation of features for training dataset\n",
        "# Accelerometer features\n",
        "train_features[:, 0, :] = np.mean(tr_msAcc, axis=1)\n",
        "train_features[:, 1, :] = np.max(tr_msAcc, axis=1)\n",
        "train_features[:, 2, :] = np.min(tr_msAcc, axis=1)\n",
        "train_features[:, 3, :] = np.std(tr_msAcc, axis=1)\n",
        "train_features[:, 4, :] = np.sum(np.diff(np.sign(tr_msAcc), axis=1) != 0, axis=1)\n",
        "train_features[:, 5, :] = np.percentile(tr_msAcc, 20, axis=1)\n",
        "train_features[:, 6, :] = np.percentile(tr_msAcc, 50, axis=1)\n",
        "train_features[:, 7, :] = np.percentile(tr_msAcc, 80, axis=1)\n",
        "\n",
        "# Gyroscope features\n",
        "train_features[:, 8, :] = np.mean(tr_msGyr, axis=1)\n",
        "train_features[:, 9, :] = np.max(tr_msGyr, axis=1)\n",
        "train_features[:, 10, :] = np.min(tr_msGyr, axis=1)\n",
        "train_features[:, 11, :] = np.std(tr_msGyr, axis=1)\n",
        "train_features[:, 12, :] = np.sum(np.diff(np.sign(tr_msGyr), axis=1) != 0, axis=1)\n",
        "train_features[:, 13, :] = np.percentile(tr_msGyr, 20, axis=1)\n",
        "train_features[:, 14, :] = np.percentile(tr_msGyr, 50, axis=1)\n",
        "train_features[:, 15, :] = np.percentile(tr_msGyr, 80, axis=1)\n",
        "\n",
        "# Generation of features for testing dataset\n",
        "# Accelerometer features\n",
        "test_features[:, 0, :] = np.mean(ts_msAcc, axis=1)\n",
        "test_features[:, 1, :] = np.max(ts_msAcc, axis=1)\n",
        "test_features[:, 2, :] = np.min(ts_msAcc, axis=1)\n",
        "test_features[:, 3, :] = np.std(ts_msAcc, axis=1)\n",
        "test_features[:, 4, :] = np.sum(np.diff(np.sign(ts_msAcc), axis=1) != 0, axis=1)\n",
        "test_features[:, 5, :] = np.percentile(ts_msAcc, 20, axis=1)\n",
        "test_features[:, 6, :] = np.percentile(ts_msAcc, 50, axis=1)\n",
        "test_features[:, 7, :] = np.percentile(ts_msAcc, 80, axis=1)\n",
        "\n",
        "# Gyroscope features\n",
        "test_features[:, 8, :] = np.mean(ts_msGyr, axis=1)\n",
        "test_features[:, 9, :] = np.max(ts_msGyr, axis=1)\n",
        "test_features[:, 10, :] = np.min(ts_msGyr, axis=1)\n",
        "test_features[:, 11, :] = np.std(ts_msGyr, axis=1)\n",
        "test_features[:, 12, :] = np.sum(np.diff(np.sign(ts_msGyr), axis=1) != 0, axis=1)\n",
        "test_features[:, 13, :] = np.percentile(ts_msGyr, 20, axis=1)\n",
        "test_features[:, 14, :] = np.percentile(ts_msGyr, 50, axis=1)\n",
        "test_features[:, 15, :] = np.percentile(ts_msGyr, 80, axis=1)\n",
        "\n",
        "print(\"Train features shape:\", train_features.shape)\n",
        "print(\"Test features shape:\", test_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYtMK-jPhhL0",
        "outputId": "c88a1731-fcc5-43af-aa3a-fcdfcba25b72"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (87, 16, 3)\n",
            "Test features shape: (90, 16, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_reshaped = np.reshape(train_features, (train_features.shape[0], train_features.shape[1] * train_features.shape[2]))\n",
        "test_features_reshaped = np.reshape(test_features, (test_features.shape[0], test_features.shape[1] * test_features.shape[2]))\n",
        "\n",
        "print(\"Train features reshaped shape:\", train_features_reshaped.shape)\n",
        "print(\"Test features reshaped shape:\", test_features_reshaped.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjRl0wF5nulQ",
        "outputId": "3c4c180f-00c4-4eb6-84c6-9466b1c16163"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features reshaped shape: (87, 48)\n",
            "Test features reshaped shape: (90, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_tensor = torch.from_numpy(train_features_reshaped).float()\n",
        "test_features_tensor = torch.from_numpy(test_features_reshaped).float()\n",
        "train_labels_tensor = torch.from_numpy(tr_labels).float().view(-1, 1)\n",
        "test_labels_tensor = torch.from_numpy(ts_labels).float().view(-1, 1)\n",
        "\n",
        "batch_size = 50\n",
        "learning_rate = 0.01\n",
        "epochs = 150\n",
        "\n",
        "#OPEN_DOOR == 0\n",
        "#RUB_HANDS == 1\n",
        "\n",
        "#Create DataLoader for batch processing\n",
        "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class BinaryClassificationModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(BinaryClassificationModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.sigmoid(self.fc4(x))\n",
        "        return x\n",
        "\n",
        "input_size = train_features_reshaped.shape[1]  # 48 features\n",
        "model = BinaryClassificationModel(input_size)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr= learning_rate, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs > 0.5).float()  # Convert sigmoid output to binary (0 or 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo22UH9YulZr",
        "outputId": "9d914a12-b0b6-4b05-e291-236e42a30ba3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/150], Loss: 1.8625\n",
            "Epoch [2/150], Loss: 1.4653\n",
            "Epoch [3/150], Loss: 0.7339\n",
            "Epoch [4/150], Loss: 0.6014\n",
            "Epoch [5/150], Loss: 0.5278\n",
            "Epoch [6/150], Loss: 0.3350\n",
            "Epoch [7/150], Loss: 0.2677\n",
            "Epoch [8/150], Loss: 0.6972\n",
            "Epoch [9/150], Loss: 1.6616\n",
            "Epoch [10/150], Loss: 0.5500\n",
            "Epoch [11/150], Loss: 0.4508\n",
            "Epoch [12/150], Loss: 0.3408\n",
            "Epoch [13/150], Loss: 0.2718\n",
            "Epoch [14/150], Loss: 0.2587\n",
            "Epoch [15/150], Loss: 0.2061\n",
            "Epoch [16/150], Loss: 0.2136\n",
            "Epoch [17/150], Loss: 0.2287\n",
            "Epoch [18/150], Loss: 0.1648\n",
            "Epoch [19/150], Loss: 0.1777\n",
            "Epoch [20/150], Loss: 0.1494\n",
            "Epoch [21/150], Loss: 0.1070\n",
            "Epoch [22/150], Loss: 0.0884\n",
            "Epoch [23/150], Loss: 0.0734\n",
            "Epoch [24/150], Loss: 0.0698\n",
            "Epoch [25/150], Loss: 0.1271\n",
            "Epoch [26/150], Loss: 0.1513\n",
            "Epoch [27/150], Loss: 0.1100\n",
            "Epoch [28/150], Loss: 0.1545\n",
            "Epoch [29/150], Loss: 0.0695\n",
            "Epoch [30/150], Loss: 0.0697\n",
            "Epoch [31/150], Loss: 0.1050\n",
            "Epoch [32/150], Loss: 0.0512\n",
            "Epoch [33/150], Loss: 0.0720\n",
            "Epoch [34/150], Loss: 0.0302\n",
            "Epoch [35/150], Loss: 0.0626\n",
            "Epoch [36/150], Loss: 0.0544\n",
            "Epoch [37/150], Loss: 0.0615\n",
            "Epoch [38/150], Loss: 0.0412\n",
            "Epoch [39/150], Loss: 0.0444\n",
            "Epoch [40/150], Loss: 0.0189\n",
            "Epoch [41/150], Loss: 0.0219\n",
            "Epoch [42/150], Loss: 0.0035\n",
            "Epoch [43/150], Loss: 0.0089\n",
            "Epoch [44/150], Loss: 0.0030\n",
            "Epoch [45/150], Loss: 0.0172\n",
            "Epoch [46/150], Loss: 0.0972\n",
            "Epoch [47/150], Loss: 0.0642\n",
            "Epoch [48/150], Loss: 0.0144\n",
            "Epoch [49/150], Loss: 0.0124\n",
            "Epoch [50/150], Loss: 0.0311\n",
            "Epoch [51/150], Loss: 0.0147\n",
            "Epoch [52/150], Loss: 0.0060\n",
            "Epoch [53/150], Loss: 0.0035\n",
            "Epoch [54/150], Loss: 0.0027\n",
            "Epoch [55/150], Loss: 0.0036\n",
            "Epoch [56/150], Loss: 0.0025\n",
            "Epoch [57/150], Loss: 0.0025\n",
            "Epoch [58/150], Loss: 0.0023\n",
            "Epoch [59/150], Loss: 0.0022\n",
            "Epoch [60/150], Loss: 0.0056\n",
            "Epoch [61/150], Loss: 0.0024\n",
            "Epoch [62/150], Loss: 0.0014\n",
            "Epoch [63/150], Loss: 0.0012\n",
            "Epoch [64/150], Loss: 0.0014\n",
            "Epoch [65/150], Loss: 0.0012\n",
            "Epoch [66/150], Loss: 0.0009\n",
            "Epoch [67/150], Loss: 0.0008\n",
            "Epoch [68/150], Loss: 0.0008\n",
            "Epoch [69/150], Loss: 0.0007\n",
            "Epoch [70/150], Loss: 0.0007\n",
            "Epoch [71/150], Loss: 0.0006\n",
            "Epoch [72/150], Loss: 0.0006\n",
            "Epoch [73/150], Loss: 0.0006\n",
            "Epoch [74/150], Loss: 0.0006\n",
            "Epoch [75/150], Loss: 0.0005\n",
            "Epoch [76/150], Loss: 0.0005\n",
            "Epoch [77/150], Loss: 0.0005\n",
            "Epoch [78/150], Loss: 0.0004\n",
            "Epoch [79/150], Loss: 0.0005\n",
            "Epoch [80/150], Loss: 0.0004\n",
            "Epoch [81/150], Loss: 0.0004\n",
            "Epoch [82/150], Loss: 0.0004\n",
            "Epoch [83/150], Loss: 0.0003\n",
            "Epoch [84/150], Loss: 0.0003\n",
            "Epoch [85/150], Loss: 0.0003\n",
            "Epoch [86/150], Loss: 0.0003\n",
            "Epoch [87/150], Loss: 0.0003\n",
            "Epoch [88/150], Loss: 0.0003\n",
            "Epoch [89/150], Loss: 0.0003\n",
            "Epoch [90/150], Loss: 0.0003\n",
            "Epoch [91/150], Loss: 0.0003\n",
            "Epoch [92/150], Loss: 0.0003\n",
            "Epoch [93/150], Loss: 0.0003\n",
            "Epoch [94/150], Loss: 0.0003\n",
            "Epoch [95/150], Loss: 0.0002\n",
            "Epoch [96/150], Loss: 0.0002\n",
            "Epoch [97/150], Loss: 0.0002\n",
            "Epoch [98/150], Loss: 0.0002\n",
            "Epoch [99/150], Loss: 0.0002\n",
            "Epoch [100/150], Loss: 0.0002\n",
            "Epoch [101/150], Loss: 0.0002\n",
            "Epoch [102/150], Loss: 0.0002\n",
            "Epoch [103/150], Loss: 0.0002\n",
            "Epoch [104/150], Loss: 0.0002\n",
            "Epoch [105/150], Loss: 0.0002\n",
            "Epoch [106/150], Loss: 0.0002\n",
            "Epoch [107/150], Loss: 0.0002\n",
            "Epoch [108/150], Loss: 0.0002\n",
            "Epoch [109/150], Loss: 0.0002\n",
            "Epoch [110/150], Loss: 0.0002\n",
            "Epoch [111/150], Loss: 0.0002\n",
            "Epoch [112/150], Loss: 0.0002\n",
            "Epoch [113/150], Loss: 0.0002\n",
            "Epoch [114/150], Loss: 0.0002\n",
            "Epoch [115/150], Loss: 0.0002\n",
            "Epoch [116/150], Loss: 0.0002\n",
            "Epoch [117/150], Loss: 0.0002\n",
            "Epoch [118/150], Loss: 0.0002\n",
            "Epoch [119/150], Loss: 0.0001\n",
            "Epoch [120/150], Loss: 0.0001\n",
            "Epoch [121/150], Loss: 0.0001\n",
            "Epoch [122/150], Loss: 0.0002\n",
            "Epoch [123/150], Loss: 0.0001\n",
            "Epoch [124/150], Loss: 0.0001\n",
            "Epoch [125/150], Loss: 0.0001\n",
            "Epoch [126/150], Loss: 0.0001\n",
            "Epoch [127/150], Loss: 0.0001\n",
            "Epoch [128/150], Loss: 0.0001\n",
            "Epoch [129/150], Loss: 0.0001\n",
            "Epoch [130/150], Loss: 0.0001\n",
            "Epoch [131/150], Loss: 0.0001\n",
            "Epoch [132/150], Loss: 0.0001\n",
            "Epoch [133/150], Loss: 0.0001\n",
            "Epoch [134/150], Loss: 0.0001\n",
            "Epoch [135/150], Loss: 0.0001\n",
            "Epoch [136/150], Loss: 0.0001\n",
            "Epoch [137/150], Loss: 0.0001\n",
            "Epoch [138/150], Loss: 0.0001\n",
            "Epoch [139/150], Loss: 0.0001\n",
            "Epoch [140/150], Loss: 0.0001\n",
            "Epoch [141/150], Loss: 0.0001\n",
            "Epoch [142/150], Loss: 0.0001\n",
            "Epoch [143/150], Loss: 0.0001\n",
            "Epoch [144/150], Loss: 0.0001\n",
            "Epoch [145/150], Loss: 0.0001\n",
            "Epoch [146/150], Loss: 0.0001\n",
            "Epoch [147/150], Loss: 0.0001\n",
            "Epoch [148/150], Loss: 0.0001\n",
            "Epoch [149/150], Loss: 0.0001\n",
            "Epoch [150/150], Loss: 0.0001\n",
            "Test Accuracy: 97.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg_classifier = LogisticRegression(solver='liblinear')\n",
        "log_reg_classifier.fit(train_features_reshaped, tr_labels)\n",
        "\n",
        "#Predict on the test data\n",
        "estimatedLabels = log_reg_classifier.predict(test_features_reshaped)\n",
        "\n",
        "#Evaluate the performance\n",
        "accuracy = accuracy_score(ts_labels, estimatedLabels)\n",
        "weightedF1 = f1_score(ts_labels, estimatedLabels, average='weighted')\n",
        "averageF1 = f1_score(ts_labels, estimatedLabels, average='macro')\n",
        "allF1Scores = f1_score(ts_labels, estimatedLabels, average=None)\n",
        "conf_matrix = confusion_matrix(ts_labels, estimatedLabels)\n",
        "\n",
        "print('Logistic Regression Results:')\n",
        "print('   Average F1-score = %.4f' % (averageF1))\n",
        "print('   Test accuracy = %.2f %%' % (accuracy * 100))\n",
        "print('   Weighted F1-score = %.4f' % (weightedF1))\n",
        "print('   All F1-scores:', allF1Scores)\n",
        "print('   Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "print('-------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRL9pp8c2ERN",
        "outputId": "188e21ca-13a2-4ee3-c06d-ca651de44363"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Results:\n",
            "   Average F1-score = 0.9333\n",
            "   Test accuracy = 93.33 %\n",
            "   Weighted F1-score = 0.9334\n",
            "   All F1-scores: [0.93478261 0.93181818]\n",
            "   Confusion Matrix:\n",
            "[[43  5]\n",
            " [ 1 41]]\n",
            "-------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}